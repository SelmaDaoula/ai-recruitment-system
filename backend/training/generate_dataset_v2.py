"""
GÃ©nÃ©rateur de Dataset SynthÃ©tique V2 - AMÃ‰LIORÃ‰
Annotations plus prÃ©cises, pas de faux positifs
"""

import random
import json
from pathlib import Path
from typing import List, Tuple

# Dataset de base (IDENTIQUE)
FIRST_NAMES = ["Jean", "Marie", "Pierre", "Sophie", "Thomas", "Julie", "Nicolas", "Emma", "Alexandre", "Camille"]
LAST_NAMES = ["Dupont", "Martin", "Bernard", "Dubois", "Lambert", "Moreau", "Simon", "Michel", "Laurent", "Leroy"]

SKILLS = {
    "langages": ["Python", "JavaScript", "Java", "C++", "PHP", "Ruby", "Go", "TypeScript", "Rust", "Kotlin"],
    "frameworks": ["Django", "Flask", "FastAPI", "React", "Vue", "Angular", "Spring", "Laravel", "Node.js", "Express"],
    "databases": ["PostgreSQL", "MySQL", "MongoDB", "Redis", "Elasticsearch", "Oracle", "Cassandra"],
    "devops": ["Docker", "Kubernetes", "Jenkins", "GitLab CI", "AWS", "Azure", "Terraform", "Ansible"],
    "tools": ["Git", "GitHub", "Jira", "Confluence", "VS Code", "IntelliJ"]
}

EXPERIENCE_TEMPLATES = [
    "J'ai {years} ans d'expÃ©rience en {skill}",
    "ExpÃ©rience de {years} ans avec {skill}",
    "MaÃ®trise de {skill} depuis {years} ans",
    "{years} annÃ©es d'expÃ©rience professionnelle en {skill}",
    "CompÃ©tent en {skill} avec {years} ans d'expÃ©rience",
    "Expert {skill} ({years} ans)",
    "Utilisation de {skill} pendant {years} ans"
]

# NOUVEAU : Templates sans mots de contexte
SKILL_TEMPLATES_CLEAN = [
    "{skills}",  # Juste les compÃ©tences
    "Stack : {skills}",
    "Technos : {skills}",
    "Outils : {skills}"
]

JOB_TITLES = [
    "DÃ©veloppeur Full Stack",
    "IngÃ©nieur Logiciel",
    "Data Scientist",
    "DevOps Engineer",
    "Lead Developer",
    "Architecte Logiciel",
    "DÃ©veloppeur Backend",
    "DÃ©veloppeur Frontend",
    "IngÃ©nieur Machine Learning"
]

COMPANIES = ["TechCorp", "DataLab", "CloudSystems", "StartupLab", "InnovateTech", "WebAgency", "CodeFactory"]

# NOUVEAU : Liste d'exclusion
EXCLUDED_WORDS = [
    "CompÃ©tences", "Langages", "Frameworks", "Technologies", "Stack", "Outils",
    "FranÃ§ais", "Anglais", "Espagnol", "Allemand", "Italien",
    "Natif", "Courant", "IntermÃ©diaire", "DÃ©butant", "Bilingue",
    "Email", "TÃ©lÃ©phone", "ExpÃ©rience", "Formation", "Langues",
    "Bases de donnÃ©es", "DevOps", "Cloud", "A1", "A2", "B1", "B2", "C1", "C2"
]


class CVDatasetGeneratorV2:
    """
    GÃ©nÃ©rateur V2 avec annotations plus prÃ©cises
    """
    
    def __init__(self, output_dir: str = "data/training"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.train_data = []
        
        # CrÃ©er la liste complÃ¨te des compÃ©tences valides
        self.valid_skills = []
        for category in SKILLS.values():
            self.valid_skills.extend(category)
    
    def generate_cv_text(self) -> str:
        """
        GÃ©nÃ¨re le texte d'un CV fictif (AMÃ‰LIORÃ‰)
        """
        name = f"{random.choice(FIRST_NAMES)} {random.choice(LAST_NAMES)}"
        job_title = random.choice(JOB_TITLES)
        
        # Header
        cv = f"{name}\n{job_title}\n\n"
        
        # ExpÃ©rience
        cv += "EXPERIENCE PROFESSIONNELLE\n\n"  # Sans accents pour Ã©viter confusion
        
        num_jobs = random.randint(1, 3)
        for _ in range(num_jobs):
            company = random.choice(COMPANIES)
            years = random.randint(1, 8)
            
            # SÃ©lectionner des compÃ©tences
            all_skills = []
            for category in SKILLS.values():
                all_skills.extend(category)
            
            job_skills = random.sample(all_skills, random.randint(3, 5))
            
            # ExpÃ©rience template
            exp_template = random.choice(EXPERIENCE_TEMPLATES)
            main_skill = job_skills[0]
            
            cv += f"{random.choice(JOB_TITLES)} - {company}\n"
            cv += exp_template.format(years=years, skill=main_skill) + "\n"
            
            # CHANGEMENT : Juste les technos, pas de prÃ©fixe
            cv += f"Technos : {', '.join(job_skills[1:])}\n\n"
        
        # CompÃ©tences (SECTION AMÃ‰LIORÃ‰E)
        cv += "COMPETENCES\n\n"  # Sans accents
        
        # SÃ©lectionner des compÃ©tences par catÃ©gorie
        selected_skills = []
        for category, skills in SKILLS.items():
            selected_skills.extend(random.sample(skills, random.randint(2, 4)))
        
        # CHANGEMENT : Template sans mots-clÃ©s problÃ©matiques
        skill_text = random.choice(SKILL_TEMPLATES_CLEAN).format(skills=", ".join(selected_skills))
        cv += skill_text + "\n\n"
        
        # Formation
        cv += "FORMATION\n\n"
        cv += "Master Informatique - Universite Paris (2019)\n"
        
        # PAS de section langues (pour Ã©viter confusion)
        
        return cv
    
    def annotate_cv_precise(self, text: str) -> Tuple[str, dict]:
        """
        Annote un CV avec PRÃ‰CISION (V2)
        
        RÃ¨gles strictes :
        1. Annoter SEULEMENT les compÃ©tences techniques valides
        2. NE PAS annoter les titres de section
        3. NE PAS annoter les langues
        4. NE PAS annoter les mots de contexte
        """
        entities = []
        
        for skill in self.valid_skills:
            # Trouver toutes les occurrences
            start = 0
            while True:
                pos = text.find(skill, start)
                if pos == -1:
                    break
                
                # VÃ©rifier que c'est un mot complet
                if pos > 0 and text[pos-1].isalnum():
                    start = pos + 1
                    continue
                
                end = pos + len(skill)
                if end < len(text) and text[end].isalnum():
                    start = pos + 1
                    continue
                
                # NOUVEAU : VÃ©rifier le contexte (50 chars avant)
                context_start = max(0, pos - 50)
                context = text[context_start:pos].lower()
                
                # Exclure si dans un contexte de langue
                language_keywords = ["franÃ§ais", "anglais", "espagnol", "allemand", "langue"]
                if any(keyword in context for keyword in language_keywords):
                    start = end
                    continue
                
                # Ajouter l'annotation
                entities.append((pos, end, "SKILL"))
                start = end
        
        # Trier par position
        entities.sort()
        
        # Supprimer les doublons
        unique_entities = []
        seen = set()
        for start, end, label in entities:
            key = (start, end)
            if key not in seen:
                unique_entities.append((start, end, label))
                seen.add(key)
        
        return (text, {"entities": unique_entities})
    
    def generate_dataset(self, n_samples: int = 500):
        """
        GÃ©nÃ¨re un dataset complet V2
        """
        print(f"\nðŸ”„ GÃ©nÃ©ration de {n_samples} CVs (V2 - QualitÃ© amÃ©liorÃ©e)...")
        
        for i in range(n_samples):
            # GÃ©nÃ©rer un CV
            cv_text = self.generate_cv_text()
            
            # Annoter avec prÃ©cision
            annotated = self.annotate_cv_precise(cv_text)
            
            self.train_data.append(annotated)
            
            if (i + 1) % 100 == 0:
                print(f"  âœ… {i + 1}/{n_samples} CVs gÃ©nÃ©rÃ©s")
        
        print(f"\nâœ… {n_samples} CVs gÃ©nÃ©rÃ©s avec succÃ¨s !")
        
        # Statistiques
        total_entities = sum(len(data[1]["entities"]) for data in self.train_data)
        print(f"ðŸ“Š {total_entities} annotations de compÃ©tences au total")
        print(f"ðŸ“Š Moyenne : {total_entities / n_samples:.1f} compÃ©tences par CV")
        
        # NOUVEAU : VÃ©rifier la qualitÃ©
        self._check_quality()
    
    def _check_quality(self):
        """
        VÃ©rifie qu'il n'y a pas de faux positifs Ã©vidents
        """
        print(f"\nðŸ” VÃ©rification de la qualitÃ© des annotations...")
        
        # Compter les annotations par mot
        word_counts = {}
        
        for text, annotations in self.train_data:
            for start, end, label in annotations["entities"]:
                word = text[start:end]
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # DÃ©tecter les mots suspects (qui ne sont pas des compÃ©tences valides)
        suspect_words = []
        for word, count in word_counts.items():
            if word in EXCLUDED_WORDS:
                suspect_words.append((word, count))
        
        if suspect_words:
            print(f"âš ï¸  {len(suspect_words)} mots suspects dÃ©tectÃ©s :")
            for word, count in suspect_words[:10]:
                print(f"   - {word} : {count} occurrences")
        else:
            print(f"âœ… Aucun mot suspect dÃ©tectÃ© !")
        
        # Top 10 compÃ©tences
        print(f"\nðŸ“Š Top 10 compÃ©tences annotÃ©es :")
        top_skills = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        for skill, count in top_skills:
            print(f"   â€¢ {skill} : {count} fois")
    
    def split_dataset(self, train_ratio: float = 0.8):
        """
        SÃ©pare en train/test
        """
        random.shuffle(self.train_data)
        
        split_idx = int(len(self.train_data) * train_ratio)
        
        train_set = self.train_data[:split_idx]
        test_set = self.train_data[split_idx:]
        
        return train_set, test_set
    
    def save_dataset(self):
        """
        Sauvegarde le dataset V2
        """
        train_set, test_set = self.split_dataset()
        
        # Sauvegarder train
        train_file = self.output_dir / "train_data_v2.json"
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_set, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸ’¾ Train set V2 sauvegardÃ© : {train_file}")
        print(f"   {len(train_set)} exemples")
        
        # Sauvegarder test
        test_file = self.output_dir / "test_data_v2.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_set, f, ensure_ascii=False, indent=2)
        
        print(f"ðŸ’¾ Test set V2 sauvegardÃ© : {test_file}")
        print(f"   {len(test_set)} exemples")
        
        # Sauvegarder un exemple pour vÃ©rification
        example_file = self.output_dir / "example_v2.txt"
        with open(example_file, 'w', encoding='utf-8') as f:
            example = train_set[0]
            f.write("TEXTE:\n")
            f.write(example[0])
            f.write("\n\nANNOTATIONS:\n")
            for start, end, label in example[1]["entities"]:
                skill = example[0][start:end]
                f.write(f"  {skill} [{start}:{end}] â†’ {label}\n")
        
        print(f"ðŸ’¾ Exemple sauvegardÃ© : {example_file}")
        
        return train_file, test_file


# ============ Script principal ============

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ðŸš€ GÃ‰NÃ‰RATEUR DE DATASET V2 (QUALITÃ‰ AMÃ‰LIORÃ‰E)")
    print("="*60)
    
    # CrÃ©er le gÃ©nÃ©rateur V2
    generator = CVDatasetGeneratorV2()
    
    # GÃ©nÃ©rer le dataset
    generator.generate_dataset(n_samples=500)
    
    # Sauvegarder
    train_file, test_file = generator.save_dataset()
    
    print("\n" + "="*60)
    print("âœ… DATASET V2 PRÃŠT !")
    print("="*60)
    print("\nAmÃ©liorations :")
    print("  âœ… Pas d'annotations sur les titres de section")
    print("  âœ… Pas d'annotations sur les langues")
    print("  âœ… Pas de mots de contexte annotÃ©s")
    print("  âœ… VÃ©rification de qualitÃ© automatique")
    print("\nProchaine Ã©tape : RÃ©-entraÃ®ner avec ce dataset amÃ©liorÃ©")
    print("\n" + "="*60 + "\n")